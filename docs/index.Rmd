---
title: "Predicting NFL wins using Machine Learning"
author: "Angus Watters"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: simplex
    highlight: tango
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE, warning = FALSE, message = FALSE) 
```

```{r, eval = TRUE, echo = FALSE}

# remove(list = ls())  # clear all workspace variables

library(rmarkdown)
library(knitr)
library(ggplot2)
library(dplyr)
library(flextable)
library(tidymodels)

source("../utils/utils.R")

nfl        <- readRDS(here::here("data", "football_wins.rds"))

# Load entire dataset
model_dat <- readRDS(here::here("data", "football_wins_lag_elo.rds")) 

pred_holdout <- readRDS(here::here("data", "holdout_predictions.rds"))
pred_table   <- readRDS(here::here("data", "holdout_predictions_table.rds"))
confmat_tbl  <- readRDS(here::here("img", "conf_mat_metrics.png"))
nfl_split       <- readRDS(here::here("data", "class_split_data.rds"))
nfl_train       <- readRDS(here::here("data", "class_train_data.rds"))
nfl_test        <- readRDS(here::here("data", "class_test_data.rds"))

wfs_ranks       <- readRDS(here::here("data", "class_wfs_ranks.rds"))

roc_df          <- readRDS(here::here("data", "win_roc.rds"))
vip_df          <- readRDS(here::here("data", "win_variable_importance.rds"))
confmat_df      <- readRDS(here::here("data", "win_conf_matrix.rds"))
metrics_df      <- readRDS(here::here("data", "win_metrics.rds"))

holdout_result  <- readRDS(here::here("data", "holdout_year_predictions.rds"))

holdout_df <-
  model_dat %>% 
  dplyr::filter(season == 2021, home == 1) %>% 
  dplyr::select(-abs_spread_line, -home, -home_fav, -spread_line) %>% 
  dplyr::select(season, week, game_id, team, opponent, fav, win, 
                div_game, rest_days, opp_rest_days, elo, opp_elo,
                score_diff, opp_score_diff, turnovers, opp_turnovers,
                win_pct, away_win_pct,
                home_win_pct,  opp_win_pct, opp_away_win_pct,
                opp_home_win_pct, qb_epa, opp_qb_epa,
                third_down_pct, opp_third_down_pct, 
                score_drives, opp_score_drives) 

holdout_test <- 
  model_dat %>% 
  dplyr::filter(home == 1, game_id %in% nfl_test$game_id) %>%
  dplyr::select(names(holdout_df)) 
# Logisitc regression model
log_reg_model      <- readRDS(here::here("data", "win_model_logistic_reg.rds"))
# log_reg_last_fit   <- readRDS(here::here("data", "win_resamples_logistic_reg.rds"))
# log_reg_last_fit %>% 
#   collect_predictions() %>%
#   conf_mat(win, .pred_class) %>%
#   autoplot("heatmap")


# XGBoost model
xgb_model <- readRDS(here::here("data", "win_model_boost_tree.rds"))

# SVM Poly model
svm_poly_model <- readRDS(here::here("data", "win_model_svm_poly.rds"))

# SVM RBF model
svm_rbf_model <- readRDS(here::here("data", "win_model_svm_rbf.rds"))

# SVM RBF model
mlp_model <- readRDS(here::here("data", "win_model_svm_poly.rds"))

# nfl_wfs <- readRDS("D:/nfl/classification/wfs/win_classification_wfs2.rds")
```


# **Introduction**
My goal with this project was to attempt to answer the age old sports question of **"So, Who is going to win this game?"** 

I wanted to see if I could use historic information about a teams offensive and defensive performances to garner an accurate prediction on whether or not they will win there upcoming game. 

All data ingestion, processing, analysis, and modeling was done in R and I leaned heavily on the `tidyverse` and `tidymodels` ecosystems to get this done. 
<br>
<br>

---

# **Data Retrieval**
I pulled NFL play-by-play data together using the `nflFastR R package`. I then did some data cleaning and wrangling tasks to get the data into a usable format. 

I started by getting the full play-by-play datasets from 1999-2021. Each year there are some ~50,000 plays run, and for each play, there are 372 columns of data giving information about what happened on that play. 

I created a handful of helper functions that take the `nflFast R` play-by-play data as an input and then aggregates the data to game level data. 

In the end I used data from the 1999-2021 seasons, however I completed omitted the entire 2021 season to use as a holdout set of data to test our models against. 

<br>

---


# **Exploritory Data Analysis**

Let's dig into some of the data and see what we can learn. 

<br>
## Home vs Away wins

```{r}
home_wins <- 
  nfl %>% 
  dplyr::filter(week < 19) %>% 
  # dplyr::group_by(season, week, home) %>% 
  dplyr::group_by(season, home) %>% 
  dplyr::mutate(
    loss = dplyr::case_when(
      win == 0 ~ 1,
      win != 0 ~ 0,
      TRUE ~ 0
    )
  ) %>% 
  dplyr::summarise(
    win  = sum(win, na.rm = T),
    loss = sum(loss, na.rm = T),
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    games = win + loss
  ) %>% 
  na.omit() %>% 
  dplyr::mutate(
    home_win_pct = round(win/games, 3)
  ) %>% 
  dplyr::filter(home == 1) 
  # dplyr::group_by()

home_wins %>% 
  ggplot() +
  geom_hline(yintercept = 0.5, size = 1.5, alpha = 0.5) +
  geom_point(aes(x = season, y = home_win_pct), size = 3) +
  ggplot2::scale_y_continuous(limits = c(0, 1),
    labels = function(x) paste0(x*100, "%")
    ) +
  labs(
    title = "How often does the home team win?",
    x = "Season",
    y = "Home winning %"
  ) +
  apatheme
```

```{r}

```


```{r}
vegas_correct <- 
  nfl %>% 
  dplyr::filter(week < 19) %>% 
  dplyr::select(season, home, win, fav, abs_spread_line) %>% 
  dplyr::mutate(
    fav_win = dplyr::case_when(
      win == 1 & fav == 1 ~ 1,
      TRUE                ~ 0
    )
    ) %>% 
  dplyr::group_by(season, home) %>%
  # dplyr::group_by(season) %>% 
  dplyr::mutate(
      games = n()
      ) %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(season) %>%
  dplyr::summarise(
    fav_win = sum(fav_win, na.rm = T)
    ) %>% 
  dplyr::left_join(
    dplyr::select(home_wins, season,home_win_pct,  games),
    by = "season"
    ) %>%
  dplyr::mutate(
    fav_win_pct = round(fav_win/games, 3)
    )

vegas_correct %>% 
  tidyr::pivot_longer(cols = c(fav_win_pct, home_win_pct)) %>% 
  ggplot() +
  geom_hline(yintercept = 0.5, size = 1.5, alpha = 0.5) +
    geom_point(aes(x = season, y = value, col = name)) +
  # geom_point(aes(x = season, y = fav_win_pct)) +
  # geom_point(aes(x = season, y = home_win_pct), size = 3) +
  ggplot2::scale_y_continuous(limits = c(0, 1),
    labels = function(x) paste0(x*100, "%")
    ) +
  labs(
    title = "How often does the favored team",
    x = "Season",
    y = "Home winning %"
  ) +
  apatheme
```
```{r}
favorites <-
  nfl %>% 
  dplyr::filter(week < 19) %>% 
  dplyr::select(season,week, home, win, fav, abs_spread_line) %>% 
  dplyr::mutate(
    home_fav = dplyr::case_when(
      home == 1 & fav == 1 ~ 1,
      TRUE                 ~ 0
    ),
    away_fav = dplyr::case_when(
      home == 0 & fav == 1 ~ 1,
      TRUE                 ~ 0
      )
    ) %>% 
  # dplyr::group_by(season, week) %>%
  dplyr::group_by(season) %>%
  dplyr::summarise(
    home_fav = sum(home_fav, na.rm = T),
    away_fav = sum(away_fav, na.rm = T)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    home_fav_pct = round(home_fav/(home_fav + away_fav), 3),
    away_fav_pct = round(away_fav/(home_fav + away_fav), 3)
    )

favorites %>% 
  tidyr::pivot_longer(cols = c(home_fav_pct, away_fav_pct)) %>% 
  ggplot() +
  geom_point(aes(x = season, y = value, col = name)) 
  geom_line(aes(x = week, y = value, col = name)) +
  # geom_col(aes(x = reorder(week, season), y = value, fill = name)) +
  facet_wrap(~season)

# hist(favorites$home_fav, breaks = 10)
# hist(favorites$away_fav, breaks = 10)
  
home_favorite <- 
  vegas_correct %>% 
  dplyr::left_join(
    dplyr::select(favorites, season, home_fav_pct),
    by = "season"
    )

  

home_favorite_plot <- 
    home_favorite %>% 
    tidyr::pivot_longer(cols = c(home_win_pct, fav_win_pct, home_fav_pct)) %>% 
    dplyr::mutate(
      name = dplyr::case_when(
        name == "home_win_pct" ~ "Win % - Home team",
        name == "fav_win_pct"  ~ "Win % - Favored team",
        name == "home_fav_pct" ~ "Win % - Favored Home team"
      )
    ) %>% 
    ggplot2::ggplot() +
    ggplot2::geom_hline(yintercept = 0.5, size = 1.5, alpha = 1, linetype = "dashed") +
    ggplot2::geom_line(aes(x = season, y = value, col = name), size = 1.5, alpha = 0.8) +
    ggplot2::geom_point(aes(x = season, y = value, col = name), size = 3, alpha = 1) +
    ggplot2::scale_x_continuous(breaks = seq(1999, 2021, 2)) +
    # scale_x_continuous(breaks = seq(2000, 2020, 2)) +
    ggplot2::scale_y_continuous(limits = c(0, 1),
      labels = function(x) paste0(x*100, "%")
      ) +
    ggplot2::labs(
      title = "How often does the home team and/or the favored team win?",
      x     = "Season",
      y     = "Winning %"
    ) +
    apatheme 
    # ggplot2::theme( 
    #   axis.title.x        = ggplot2::element_text(size = 14, hjust=0.5,  vjust = -2),
    #   axis.title.y        = ggplot2::element_text(size = 16, hjust=0.5, vjust = 2),
    #   axis.text.x = ggplot2::element_text(size = 14),
    #   axis.text.y = ggplot2::element_text(size = 14)
    #   # axis.text = ggplot2::element_text(size = 14)
    #   # panel.grid.minor.x  = ggplot2::element_blank()
    #   )
home_favorite_plot

ggplot2::ggsave(
    here::here("img", "home_favorite_win_pct.png"),
    home_favorite_plot,
    width  = 14,
    height = 10
  )
```

## Season win totals

Below is a box plot showing the season win totals for each NFL team over the period of record, arranged by mean annual win total. It is not surprising to see the New England Patriots leading the league with an average of 12.6 wins per season. The Patriots are followed by the Steelers, Packers, Colts, Ravens, all teams that have seen consistent success over the last 23 years. At the bottom of the league are the Browns, Lions, Jaguars, and the Raiders. 

<center>
![](../img/team_win_totals.png)
</center>

<br>
<br>

## QB performances

Now how about which team's quarterbacks have the highest EPA. EPA is a metric that determines how likely a team is to score points as a result of a play. If a QB does something good (long completion, run for a first down), then there team is more likely to score points on the ensuing play, thus Expected points were added (EPA is positive). On the other hand if a QB does something bad (incompletion, takes a big sack), then there team is less likely to score points on the ensuing play and expected points were removed (EPA is negative)

Below is a plot showing the for each team, the average QB EPA across the all seasons. It is not suprising to see teams like the Patriots, Packers, and Colts at the top of the league in terms of average QB EPA seeing as though these teams had Hall of Fame Quarterbacks for most of the period of time, Tom Brady, Brett Favre/Aaron Rodgers, and Peyton Manning, respectively. This plot matches very closely with the above season win totals boxplot. The teams with the best quarterbacks tend to have the most total wins at the end of a season. 

<center>
![](../img/qb_epa.png)
</center>

<br>
<br>

---

# **Feature selection**


I decided I wanted to capture 4 key domains of features that may influence a teams likelihood to win an upcoming game:

- Elo Rating
- Offense
- Defense
- Off field factors

**Note:** While exploring the data and creating the features I wanted to use in the model, I created a host of utility helper functions that work off of the play-by-play output data from the `nflFastR` package. Those helper functions can be found [here](https://github.com/anguswg-ucsb/nfl_wins/blob/main/utils/utils.R).

<br>

## Elo Rating
I created an Elo rating system for each season to create a metric that keeps track of a teams rank relative to the rest of the league. Elo rating systems were first created to rate chess players and are now commonly used in many sports such as American Football, baseball, basketball, etc. Special thanks to the creators of the [elo](https://eheinzen.github.io/elo/) package, your package made my life a lot easier. Here is more information on [Elo Rating Systems](https://en.wikipedia.org/wiki/Elo_rating_system) and its inventor [Arpad Elo](https://en.wikipedia.org/wiki/Arpad_Elo). 


<center>
![](../img/team_elo_ratings.png)
</center>

<br>


## Offense
 I calculated offensive and defensive statistics for every game a team played in the NFL from 1999 - 2021.


```{r, eval = TRUE, echo=FALSE}


off_table <- tibble::tibble(
  Variable    = c("Score Differential",
                  "Third Down Conversion Rate", 
                  "Turnovers",
                  "QB EPA",
                  "Scoring Drive percentage"
                  ),
  Type        = c("Numeric", "Numeric","Numeric", "Numeric", "Numeric"),
  Description = c("Average score differential throughtout the game",
                  "Percent of 3rd downs converted by offense",
                  "Total Turnovers by the offense (Fumbles lost + Interceptions",
                  "Average Quarterback EPA across all plays in the game",
                  "Number of drives that results in a score (touchdown/fieldgoal)"
                  )
)

kableExtra::kable(off_table)
```

<br>

## Defense
 I replicated many of the same offensive statistics for each team's defense to represent how opposing offenses performed against there defense. 


```{r, eval = TRUE, echo=FALSE}

def_table <- tibble::tibble(
  Variable    = c("Opponent Points Scored in quarter 1, 2, 3, 4", 
                  "Opponent Third Down Conversion Rate", 
                  "Opponent Turnovers",
                  "Opponent QB EPA",
                  "Opponent Scoring Drive percentage"
                  ),
  Type        = c("Numeric", "Numeric", "Numeric", "Numeric","Numeric"),
  "Model Variable" = c("def_qtr_pts_1,2,3,4", "def_third_down_pct", "def_turnovers", "def_qb_epa", "def_score_drives_pct"),
  Description = c("Total Points allowed by defense at the end of each quarter",
                  "Percent of 3rd downs converted by opposing offenses",
                  "Total Turnovers created by defense (Fumbles lost + Interceptions",
                  "Average Quarterback EPA allowed by defense across all plays in the game",
                  "Percent of Drives against the defense that resulted in points")
)

kableExtra::kable(def_table)
```

<br>

## Off field factors

I included a handful of other information known prior to the game such as if the game is a home game, a division game, the number of days of rest between games, and the team's overall, home, and away winning percentages, and the team's ranking via an Elo rating. 

```{r, eval = TRUE, echo=FALSE}
other_fact_table <- tibble::tibble(
  Variable    = c("Home", "Division Game", "Rest days",
                  "Win %", "Home Win %", "Away Win %", "Elo rating"
                  ),
  Type        = c("Binary", "Binary", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric"),
"Model Variable" = c("home", "div_game", "rest_days", "win_pct", "home_win_pct", "away_win_pct", "elo"),
  Description = c("Indicating if the team is the home team (1 = home, 0 = away",
                  "Indicating if it is a division game (1 = division game, 0 = not division game",
                  "Numeric variable for the amount of rest the team had between games",
                  "Percent of all games won",
                  "Percent of home games won",
                  "Percent of away games won",
                  "Elo rating per week in each season, relative to rest of the league"
                  )
)

kableExtra::kable(other_fact_table)
```


Once all of the above variables were wrangled, cleaned, and aggregated from the NFL play-by-play data, I was left with a data frame that had 1 row for every game a team played during the season, with a column for each of the aforementioned derived variables.

## Corrolation
Let's take a look at how our game summary data relates to wins. 

<br>

Below is a correlation matrix that highlights the relationship between variables in our data. We are most interested in the bottom row in this plot which indicates the correlation between a team winning there game and the other variables in our data set. Darker green colors indicate positive correlations while darker red colors indicate negative correlations. 

We see that Elo rating has a relatively strong positive correlation with winning, which makes alot of sense, a higher Elo rating (higher ranked team) is more likely to end up with a win. The average score differential for a team also makes good sense, if a team on average has a higher score differential (they score more points than are scored against them) then it's logical that they would end up winning more games. The turnover variable has a strong negative correlation with winning, so if a team on average has fewer turnovers, they are more likely to end up winning the game. 

<center>
![](../img/correlation_home_team.png)
</center>

<br>

At this point in my analysis, I was working with ***a posteriori*** data, so data that was collected as a product of what happened on the field that week. If I want to make any useful predictions about the coming week, I would need to conjur up some ***a priori*** data.

<br>

## Cumulative Averages

To capture how well a team is doing throughout each season, I created lagged cumulative means for all of the above variables. Such that, for each week a team plays a game, the cumulative mean of all the ***preceeding*** weeks of data are calculated for every variable leading into the upcoming slate of games.

Below is the general method I used to get these lagged Cumulative averages values. Note, for some variables like winning percentage, the lagged cumulative averages was ***not*** calculated and rather only the lagged value was determined. In the example code below, the lagged winning percentage, and the lagged cumulative average QB EPA is calculated for the Arizona Cardinals 2014 season. 

```{r, eval = TRUE, echo = TRUE}
nfl %>% 
  dplyr::filter(season == 2014, team == "ARI") %>% 
  dplyr::select(season, week, team, win, win_pct, qb_epa) %>% 
  dplyr::group_by(season, team) %>% 
  dplyr::arrange(season, week, .by_group = T) %>% 
  dplyr::mutate(
    across(c(win_pct), ~dplyr::lag(.x), .names = "{col}_lag"),
    across(c(qb_epa),  ~dplyr::lag(dplyr::cummean(.x)), .names = "{col}_lag")
    ) %>% 
  dplyr::mutate(across(c(win_pct:qb_epa_lag), round, 2)) %>% 
  dplyr::ungroup() %>% 
  dplyr::relocate(season, week, team, win, win_pct, win_pct_lag, qb_epa, qb_epa_lag)

```

If you take a look at ARI's week 5 game (row 5), the **win_pct_lag** column for week 5 is represented by the teams **win_pct** from the previous week, week 4 in this case, or in other words, the team's winning percentage ***leading into*** the week 5 match up. On the other hand, the  **qb_epa_lag** column was calculated using the mean of **qb_epa** from weeks 1-4. 

<br>

I did this lagging process across all of my variables such that for each row with a game outcome (win/loss), that team has variables representing their performances up until that point in the season.

<br>

Now our data is set up so that all the information used to inform our prediction is data that would be available to us ***prior*** to the upcoming week of games. This is important because information such as the number of turnovers during the game is not information we would have prior to the game, and thus can't be used as a predictor in our model, we can only use historic data to inform our predictions. 

<br>

## Home team

Initially I had been running models with 2 data points for each game, one being the home team perspective and the other being the away team perspective. I pretty quickly ran into some major overfitting in my models and realized **it was a better idea to only use data from the home team.** 

<br>

---

# **Modeling**
I decided I wanted to run my data across a panel of 6 different models and see how they all perform against each other 

  - [Logistic Regression](https://parsnip.tidymodels.org/reference/details_logistic_reg_glmnet.html)
  - [K-nearest neighbors](https://parsnip.tidymodels.org/reference/details_nearest_neighbor_kknn.html)
  - [Gradient Boosted Decision Trees](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html)
  - [Multilayer Perceptron](https://parsnip.tidymodels.org/reference/details_mlp_nnet.html)
  - [Radial basis function Support Vector Machines](https://parsnip.tidymodels.org/reference/details_svm_rbf_kernlab.html)
  - [Polynomial Support Vector Machines](https://parsnip.tidymodels.org/reference/details_svm_poly_kernlab.html)

## Data Budget/Splits
The first step was to split up our data into testing and training splits (75% training, 25% testing). When I split my data, I chose to stratify the data by the binary win/loss value to ensure there was the same proportion of wins and losses in our training and testing data splits. 

```{r, echo = TRUE, eval = FALSE}
# Set random seed
set.seed(234)

# Partition training/testing data, stratified by win/loss
nfl_split <- rsample::initial_split(nfl_df, strata = win)

# training data split - 75%
nfl_train <- rsample::training(nfl_split)

# testinng data split - 25%
nfl_test  <- rsample::testing(nfl_split)
```

<br>

## Data Preprocessing
Using the `recipes` package, I made ID variables for information about the game and when it happened. I then applied some recipe steps to my training data. Because I decided to only use the home teams data, this meant that there was going to be more wins than losses in my dataset because the home team tends to win more often then the away team (56% wins / 44% losses). To account for the slight imbalance of wins to losses in the data, I chose to use the `themis` package to upsample my data using the `themis::step_smote()` function. 

<br>

### Recipe steps
- `step_zv` is applied to all predictors to remove all variables with only one variable (zero-variance)
- `step_normalize` is used to normalize all the numeric predictors in the training data so that they all have a standard deviation of 1 and a mean of 0 
- `step_smote` is applied to our dependent **win** variable to fix the class imbalance due to having slightly more wins than losses in the data. The `step_smote` function works by upsampling the minority class and generating new, synthetic data by using the nearest-neighbors of the minority class instances. 


Below I create the two recipe steps that we will apply to our training data before modeling. 
```{r, echo = TRUE, eval = FALSE}
# Base recipe
base_recipe <- 
  recipes::recipe(
  formula = win ~ ., 
  data    = nfl_train
  ) %>% 
  recipes::update_role(
    game_id, team, opponent, season, week, new_role = "ID"
  ) 

# Normalize, SMOTE algo upsampling recipe
norm_smote_recipe <- 
  base_recipe %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_numeric_predictors())
  themis::step_smote(win, over_ratio = 0.9,  skip = T)
  
# zero variance SMOTE algo upsampling recipe
zv_smote_recipe <- 
  base_recipe %>% 
  recipes::step_zv(all_predictors()) %>% 
  themis::step_smote(win, over_ratio = 0.9,  skip = T)
```

<br>

## Cross-validation folds

I created 10 cross-validation folds from the training data. As I did with the initial training/testing split, I made sure to create **stratified resamples using the win/loss variable to ensure the same proportion of wins and losses appear in CV folds as the do in the original data.**

```{r, echo = TRUE, eval = FALSE}
# Set random seed 
set.seed(432)

# Cross-validation folds
nfl_folds <- rsample::vfold_cv(nfl_train, v = 10, strata = win)
```

<br>

## workflowsets
Using the `workflowsets` I created a `workflow_set` object containing each of the data preprocessing recipes and corresponding model specifications.

```{r, echo = TRUE, eval = FALSE}
# Workflow set of candidate models
nfl_wfs <-
  workflowsets::workflow_set(
    preproc = list(
      kknn_rec        = norm_smote_recipe,
      glmnet_rec      = norm_smote_recipe,
      xgboost_rec     = zv_smote_recipe,
      nnet_rec        = norm_smote_recipe,
      svm_poly_rec    = norm_smote_recipe,
      svm_rbf_rec     = norm_smote_recipe
    ),
    models  = list(
      knn            = knn_spec,
      glmnet         = glmnet_spec,
      xgboost        = xgboost_spec,
      nnet           = nnet_spec,
      svm_poly       = svm_poly_spec,
      svm_rbf        = svm_spec
    ),
    cross = F
  )
```

<br>

## Tuning Hyperparameters
The next step was to tune our hyperparameters for each of models. Using the `tune` package I applied the `tune::tune_grid()` function across my workflowset object containing my model recipes and specifications. 20 random candidate parameters were created using the `dials::grid_latin_hypercube()` function from the `dials` package 

### Parallel processing
This step is the most time consuming and resource intensive process of a machine learning workflow. So, to speed up the process I run the tuning process across multiple cores on my computer, making use of my computer's multiple processors. 

### Racing methods
To get another boost in processing time, I made use of the `tune_race_anova` from the `finetune` package, which makes use of a repeated measure ANOVA model to iterativily eliminates tuning parameters that are unlikely to yield the best results.  Combining **parrallel processing** and **racing methods** can improve processing times by [~35 fold](https://finetune.tidymodels.org/reference/tune_race_anova.html). 

```{r, eval = FALSE, echo = TRUE}

# Choose metrics
my_metrics <- yardstick::metric_set(roc_auc, pr_auc, accuracy, mn_log_loss)

# Set up parallelization, using computer's other cores
parallel::detectCores(logical = FALSE)
modeltime::parallel_start(6, .method = "parallel")

# Set Random seed
set.seed(589)

# Tune models in workflowset
nfl_wfs <-
  nfl_wfs %>%
  workflowsets::workflow_map(
    "tune_grid",
    resamples = nfl_folds ,
    grid      = 20,
    metrics   = my_metrics,
    control   = tune::control_grid(
      verbose   = TRUE,
      save_pred = TRUE),
    verbose   = TRUE
  )

  # Stop parrallelization
modeltime::parallel_stop()
```

<br>
<br>

---

# **Model Evaluation** 

Now we can compare how all of our models did compared to one another and make a decision as to which one we want to use to make predictions. The plots below shows how the 6 models performed on the set of resampling data. 

<center>
![](../img/class_model_ranks.png)
</center>

If you take a look at the **ROC AUC** and **mean log loss plots** the best performing models are the Logistic Regression (logistic_reg), the Support Vector Machines (svm_poly/svm_rbf), the Multilayer Perceptron (mlp) and the Gradient Boosted Decision Trees (boost_trees). 

I decided to set aside the Multilayer Perceptron model due to the slight increase in mean log loss relative to the other top performing models. 

In particular, the boosted trees looks to perform at approximately the same level asthe mlp model when it comes from correctly predicting wins (sensitivity) and correctly predicting losses (specificity), without the increase in log loss. 

<br>

A standard threshold for log loss when it comes to binary predictions is ~0.69 because anything higher than that and you aren't beating the probability of a 50-50 guess. The table below shows the mean log loss for each model. The best log loss you can have is 0 and the larger the number gets the worse the model is at predicting the correct outcome.

The table belows shows the performance metrics for how each model.

```{r, eval = TRUE, echo=FALSE}
resamp_metrics <-
  wfs_ranks %>%
  dplyr::filter(.metric %in% c("accuracy", "roc_auc", "sensitivity", "specificity" ,"mn_log_loss")) %>% 
  dplyr::group_by(model, .metric) %>% 
  dplyr::slice(1) %>% 
  dplyr::ungroup() %>% 
  tidyr::pivot_wider(
    id_cols     = c(model, rank),
    names_from  = .metric, 
    values_from = mean
    ) %>% 
  dplyr::rename(mean_log_loss = mn_log_loss) %>%
  dplyr::arrange(rank) %>% 
  dplyr::mutate(across(where(is.numeric), round, 3)) 

# Flextable of metrics
resamp_metrics %>% 
  flextable::flextable() %>% 
  flextable::add_header_row(
    values    = c("", "", "Resample Metrics"),
    colwidths = c(1, 1, 5)
    ) %>%
  flextable::theme_box() %>%
  flextable::align(align = "center", part = "all")
```


```{r}
resamp_metrics %>% 
  dplyr::arrange(mean_log_loss) %>% 
  ggplot() +
  geom_point(aes(x = reorder(model, mean_log_loss), y = mean_log_loss), size = 2) +
  geom_hline(yintercept = 0.69, size = 1.5) 
```

<br>

## Variable Importance
Looking at at variable importance plots, generated from the `vip` package, can give us an idea of which variables are the most important to ours models. 

Below is the variable importance scores for the Logistic Regression, XGBoost, and Multilayer Perceptron models. If you want to read more about what makes up the variable importance score you can click here. 

<center>
![](../img/vip_logistic_reg.png)
</center>

<center>
![](../img/vip_boost_tree.png)
</center>

<br>

The most important variables in both models looks to be the team's Elo ratings, the cumulative average score differential and the opponent's winning percentage. 

```{r, eval = FALSE, echo = FALSE}
library(patchwork)
# xgb_vip <-
mlp_vip <- 
# log_reg_vip <-
  vip_df %>%
      dplyr::filter(model == "mlp") %>%
  # dplyr::filter(model == "logistic_reg") %>%
    # dplyr::filter(model == "boost_tree") %>%
  ggplot() +
  geom_col(aes(x = Importance, y = reorder(Variable, Importance))) +
  ggplot2::labs(
    # title = "Logistic Regression",
    title = "Multilayer Perceptron",
    subtitle = "Variable Importance",
    # title = "Variable Importance",
    # subtitle = "Logistic Regression",
    # y = "Variable"
        y = ""
    ) +
  apatheme
xgb_vip
log_reg_vip
vip_plots <- log_reg_vip + xgb_vip + mlp_vip

ggsave(
    here::here("img", "var_importance.png"),
    vip_plots,
    width  = 16,
    height = 6
  )
```



```{r, eval = FALSE, echo = FALSE, fig.align="center", fig.width=8, fig.height=6}

# log_reg_vip + xgb_vip
# for (i in 1:length(unique(vip_df$model))) {
#   # unique(vip_df$model)[1]
#   vip_plot <- 
#     vip_df %>% 
#     dplyr::filter(model ==  unique(vip_df$model)[1]) %>%
#      dplyr::mutate(
#       model = dplyr::case_when(
#         model == "logistic_reg" ~ "Logistic Regression",
#         model == "boost_tree"   ~ "XGBoost",
#         model == "mlp"          ~ "MLP",
#       )
#       # model = factor(model, levels = c("Logistic Regression", "XGBoost", ))
#     ) %>% 
#     ggplot2::ggplot() +
#     ggplot2::geom_col(aes(x = Importance, y = reorder(Variable, Importance))) +
#     ggplot2::facet_wrap(~model, scales = "free_x") +
#     # ggplot2::facet_wrap(~model) 
#     ggplot2::labs(
#       title = "Variable Importance",
#       y     = "Variable"
#     ) + 
#     apatheme 
#   # +
#   # ggplot2::theme(
#   #   axis.text.y =  element_text(size = 8),
#   #   axis.text.x =  element_text(size = 8)
#   # )
  # ggsave(
  #   paste0(here::here("img"), "/vip_", unique(vip_df$model)[i], ".png"),
  #   vip_plot,
  #   width  = 12,
  #   height = 10
  # )
# }


```

<br> 
<br>

# **Model Performance on the Test Data**
I then selected the tuning parameters for each model that had the best performance metrics on the data resamples. Using these model parameters, I refit each model for the last time using just the initial data split, fitting one last time on the training data and evaluating on the testing data. I then used the final fitted models to make predictions. Let's check out the results!

```{r, eval = FALSE, echo=FALSE}

# train_test_plot <-
  metrics_df %>% 
  dplyr::filter(!.metric %in% c("accuracy", "kap")) %>% 
  dplyr::group_by(model, .metric) %>% 
  dplyr::mutate(
    model = factor(model, levels = rev(c("nearest_neighbor","logistic_reg", "boost_tree", "mlp", "svm_poly", "svm_rbf"))),
    data = factor(data, levels = c("train","test")),
    .metric = dplyr::case_when(
      # .metric == "roc_auc	"    ~ "ROCAUC",
      .metric == "mn_log_loss" ~ "Mean Log Loss",
      .metric == "sens"        ~ "Sensitivity",
      .metric == "spec"        ~ "Specificity"
    )
  ) %>% 
  replace(is.na(.), "ROC AUC") %>% 
  dplyr::mutate(across(where(is.numeric), round, 2)) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_line(aes(y = model, x = .estimate), size = 1.5) + 
  ggplot2::geom_point(aes(y = model, x = .estimate, col = data), size = 3) +
  ggplot2::facet_wrap(~.metric) +
  ggplot2::scale_x_continuous(limits = c(0.55, 0.9), breaks = seq(0.55, 0.9, 0.1)) +
  labs(
    title = "Final Fitted Model Performance", 
    subtitle = "Training/Testing data",
    color   = "Data Split",
    x     = "",
    y = "Models"
  ) + 
  apatheme 
  # ggplot2::theme(
  #   legend.title = element_text(size = 10)
  # )
# train_test_plot
#   ggsave(
#     # paste0(here::here("img"), "/vip_", unique(vip_df$model)[i], ".png"),
#     here::here("img", "train_test_metrics.png"),
#     train_test_plot,
#     width  = 12,
#     height = 10
#   )
```


<center>
![](../img/train_test_metrics.png)
</center>

<br>

And in table form... 
```{r, eval = TRUE, echo=FALSE}
metrics_df %>% 
  dplyr::filter(.metric != "kap") %>% 
  dplyr::group_by(model, .metric) %>% 
  tidyr::pivot_wider(
      id_cols     = c(model, data),
      names_from  = .metric, 
      values_from = .estimate
    ) %>% 
  dplyr::mutate(across(where(is.numeric), round, 2)) %>% 
  dplyr::rename(mean_log_loss = mn_log_loss, Model = model,  "Data split" = data, sensitivity = sens, specificity = spec) %>% 
  flextable::flextable() %>% 
  flextable::add_header_row(
    values    = c("", "", "Best model metrics"),
    colwidths = c(1, 1, 5)
    ) %>%
  flextable::theme_box() %>%
  flextable::align(align = "center", part = "all") %>% 
  flextable::hline(i = c(2, 4, 6, 8, 10), 
                   part = "body",
                   border = officer::fp_border(color = "black", width = 3)) %>% 
  flextable::hline(i = c(2), 
                   part = "header",
                   border = officer::fp_border(color = "black", width = 2))
```

<br>

The nearest neighbor model sees a big drop in mean log loss between the training and testing data, besides this, the remaining models appear to perform similarly with both the training and testing data, which gives us some more confidence that these models would perform similarly in the real world. 

<br>
<br>

## ROC Curves
Receiver Operator Characteristic curves, (aka ROC curves) are a way of understanding how well a model is predicting the True Positive events vs. the False Positive events. 

In our scenario, with models that are trying to predict if an NFL team will win, a **True Positive event would be that the model predicts a win and the team actually wins, while in the False Positive event the model predicts a win, and the team actually loses**. 

The True Positive Rate (TPR) is on the Y axis and the False Positive Rate (FPR) is on the X axis. 

The TPR, or **sensitivity**, along the Y axis indicates the probability that the model predicted a win and the team actually won. While FPR or, **1 - specificity**, along the X axis indicates the probability that the model predicted a win and the team actually lost. 

The closer the ROC curve is to the top left corner of the plot the better the model is doing at correctly classifying the data. A perfect ROC Curve would be a 90 degree angle along the top left corner of the plot and would indicate that the model is able to perfectly classify the data correctly. 


```{r, eval = FALSE, echo = TRUE, fig.align="center"}
roc_curve <-
  roc_df %>%
  ggplot2::ggplot() +
  ggplot2::geom_abline(
    lty   = 2, 
    alpha = 0.7,
    color = "black", 
    size  = 1)  +
  ggplot2::geom_line(
    ggplot2::aes(
      x     = 1 - specificity, 
      y     = sensitivity,
      color = model), 
    alpha = 0.6, 
    size  = 1
    ) +
  # ggplot2::scale_color_manual(values = RColorBrewer::brewer.pal(6, "Dark2")) +
  ggplot2::scale_color_manual(values = c( "red3", "forestgreen", "dodgerblue3", "coral3", "cyan3", "darkorange")) +
  ggplot2::coord_equal() +
  ggplot2::labs(
      title    = "ROC Curves",
      subtitle = "Test data",
      x        = "False Positive Rate (1 - Specificity)",
      y        = "True Positive Rate (Sensitivity)",
      col      = "Models"
    )  +
  apatheme 

# ggsave(
#     here::here("img", "roc_curve.png"),
#     roc_curve,
#     width  = 12,
#     height = 10
#   )

```

<center>
![](../img/roc_curve.png)
</center>

<br>

No surprises here, using the test data all of the models are performing similarly, except for the K-nearest neighbors model, which is much worse at classifying the data then the rest of the models. This ROC curve just visually confirms what we saw in the model metric summary table. 
Based on these metrics, I am going to drop the K nearest neighbors model for now and focus on the rest of the models 

<br>

## Confusion Matrices
A confusion matrix is a helpful way of understanding where a classification model is correct or incorrect in it's predictions. At first glance, I thought the name confusion matrix came from the fact that *I was confused*. Turns out the name has more meaning, its purpose is to highlight *where a classification model is confused in its predictions*...not when the modeler is confused...

<br>

So, if this is your first time seeing a confusion matrix this illustration might help.  

**Insert hot dog confusion matrix image**

```{r, eval = FALSE, echo = FALSE, fig.align="center", fig.width=6, fig.height=6}
# confusion matrix
cm_df <-
  holdout_test %>%
  dplyr::select(win, fav) %>% 
  yardstick::conf_mat(truth = win, fav) %>% 
  .$table %>% 
  as.data.frame() %>%
  dplyr::mutate(
    goodbad = ifelse(Prediction == Truth, "good", "bad"),
    prop    = Freq/sum(Freq),
    Prediction = dplyr::case_when(
    Prediction == 1 ~ "Model Predicts win",
    Prediction == 0 ~ "Model Predicts loss"
    ),
    Truth = dplyr::case_when(
      Truth == 1 ~ "Actual win",
      Truth == 0 ~ "Actual loss"
      ),
    model = "vegas"
    ) %>% 
    dplyr::bind_rows(confmat_df) %>% 
    dplyr::filter(model %in% c("vegas", "logistic_reg", "svm_rbf", "mlp", "boost_tree")) %>%
    dplyr::mutate(
      model = dplyr::case_when(
        model == "logistic_reg"       ~ "Logisitc Reg.",
        # model == "svm_poly"           ~ "SVM Poly",
        model == "boost_tree"         ~ "XGBoost",
        model == "vegas"              ~ "Vegas",
        model == "svm_rbf"            ~ "SVM RBF",
        model == "mlp"                ~ "MLP"
        # model == "nearest_neighbor"   ~ "KNN"
        ),
      Prediction = gsub("Model ", "", Prediction),
      Prediction = factor(Prediction, levels = c("Predicts win", "Predicts loss")),
      # model      = factor(model, levels = c("Vegas", "Nearest Neighbor", "Logisitc Reg.","XGBoost", "MLP", "SVM Poly", "SVM RBF"))
      model      = factor(model, levels = c("Vegas", "Logisitc Reg.", "SVM RBF", "XGBoost", "MLP"))
  ) %>% 
  dplyr::mutate(
    cm = dplyr::case_when(
      Prediction == "Predicts win"  & Truth == "Actual win"   ~ "TP",
      Prediction == "Predicts loss" & Truth == "Actual loss"  ~ "TN",
      Prediction == "Predicts win"  & Truth == "Actual loss"  ~ "FN",
      Prediction == "Predicts loss" & Truth == "Actual win"   ~ "FP"
    ),
     Freq_txt = paste0(cm, " = ", Freq)
    # Freq_txt = paste0(Freq, "\n(", cm, ")")
  )
cm_plot <- 
  cm_df %>% 
  ggplot2::ggplot(ggplot2::aes(
    x = Prediction,
    y = Truth,
    fill = goodbad)) +
  ggplot2::geom_tile(ggplot2::aes(alpha = prop)) +
  # ggplot2::facet_wrap(~model) +
  # ggplot2::facet_grid(model~.) +
  ggplot2::facet_grid(~model) +
  ggplot2::scale_fill_manual(values = c(good = "#71CD5F", bad = "red")) + 
  ggplot2::geom_text(ggplot2::aes(
    label = sprintf("%1.0f", Freq)
    # label = Freq_txt
    ),
    vjust = 1) +
  ggplot2::labs(
    title    = "Confusion matrix",
    subtitle = "Test data",
    x        = "Prediction",
    y        = "Truth"
    ) +
  apatheme +
  ggplot2::theme(
    legend.position  = "none"
    )
#     # axis.title       = ggplot2::element_text(size = 12),
#     axis.title.x     = ggplot2::element_text(hjust=0.5,  vjust = -2),
#     axis.title.y     = ggplot2::element_text(hjust=0.5, vjust = 2),
#     axis.text.y      = ggplot2::element_text(size = 10),
#     axis.text.x      = ggplot2::element_text(size = 10, angle = 0)
#       )
# cm_plot
# ggsave(
#     here::here("img", "cm_plot.png"),
#     cm_plot,
#     width  = 16,
#     height = 6
#   )

```

<center>
![](../img/cm_plot.png)
</center>

<br>

**A few notes from looking at this confusion matrix:**
- All three of my models do a better job of correctly predicting losses in the test data than Las Vegas did in the same games

- The Support Vector Machine has more total correct predictions than both other models

- The SVM RBF Model is the highest sensitivity model, it correctly predict wins better than the all other models as well as Las Vegas's predictions

- The Logistic Reg. Model predicts losses better than the SVM and XGBoost models

- ~35% of correct predictions are losses, the remaining ~65% are correctly predicted wins (~43% of the test data were actual losses so this isn't ***too*** far off)
- For this data/domain, having the model predict more wins than losses makes logical sense, **the home team tends to win more often than they lose**
- In general, the models have the most trouble classifying FP 

<br>

And here are the summary statistics for these confusion matrices

```{r, eval = FALSE, echo = FALSE, fig.align="center", fig.width=6, fig.height=6}
confmat_tbl <-
  holdout_test %>%
  dplyr::select(win, fav) %>% 
  yardstick::conf_mat(truth = win, fav) %>% 
  .$table %>% 
  as.data.frame() %>%
  dplyr::mutate(
    goodbad = ifelse(Prediction == Truth, "good", "bad"),
    prop    = Freq/sum(Freq),
    Prediction = dplyr::case_when(
    Prediction == 1 ~ "Model Predicts win",
    Prediction == 0 ~ "Model Predicts loss"
    ),
    Truth = dplyr::case_when(
      Truth == 1 ~ "Actual win",
      Truth == 0 ~ "Actual loss"
      ),
    model = "vegas"
    ) %>% 
  dplyr::bind_rows(confmat_df) %>% 
#   confmat_df %>%
  dplyr::filter(model %in% c("vegas", "logistic_reg", "svm_rbf", "boost_tree", "mlp")) %>%
  dplyr::mutate(
    model = dplyr::case_when(
      model == "vegas"            ~ "Vegas",
      model == "logistic_reg"     ~ "Logisitc Reg.",
      model == "svm_rbf"          ~ "SVM RBF",
      model == "boost_tree"       ~ "XGBoost",
      model == "mlp"              ~ "MLP"
      # model == "nearest_neighbor" ~ "KNN"
    ),
    Prediction = gsub("Model ", "", Prediction),
    Prediction = factor(Prediction, levels = c("Predicts win", "Predicts loss")),
    model      = factor(model, levels = c("Vegas", "Logisitc Reg.", "SVM RBF", "XGBoost", "MLP"))
  ) %>% 
  dplyr::mutate(
    cm = dplyr::case_when(
      Prediction == "Predicts win" & Truth == "Actual win" ~ "TP",
      Prediction == "Predicts loss" & Truth == "Actual loss" ~ "TN",
      Prediction == "Predicts win" & Truth == "Actual loss" ~ "FN",
      Prediction == "Predicts loss" & Truth == "Actual win" ~ "FP"
    ),
     Freq_txt = paste0(cm, " = ", Freq)
  ) %>% 
  tidyr::pivot_wider(
    id_cols     = c(model),
    names_from  = cm, 
    values_from = Freq
    ) %>% 
  dplyr::mutate(
    Accuracy          = round((TP + TN) / (TP + TN + FP + FN), 2),
    Misclassification = round((FP + FN) / (TP + TN + FP + FN), 2),
    Precision         = round((TP) / (TP + FP), 2),
    Sensitivity       = round((TP) / (TP + FN), 2),
    Specificity       = round((TN) / (TN + FP), 2)
    # f1_score          = round(2*((Precision * Sensitivity) / (Precision + Sensitivity)), 2)
  ) %>%
  dplyr::rename("Model" = "model") %>% 
  dplyr::select(-TP, -FP, -FN, -TN) %>% 
  dplyr::arrange(Model)

```

```{r, eval = TRUE, echo = TRUE, fig.align="center", fig.width=6, fig.height=6}
confmat_tbl %>%
  flextable::flextable() %>% 
  flextable::add_header_row(
    values    = c("", "Metrics"),
    colwidths = c(1, 5)
    ) %>%
  flextable::theme_box() %>% 
  flextable::align(align = "center", part = "all")

```

<br>

Looking at the SVM model in the table above, we see a **sensitivity** of ~0.72, which means the SVM model ***correctly predicted the home team to win 72% of the time.*** The **specificity** of 0.73 indicates that ***73% of home team losses were correctly predicted***. Among all the models, the SVM model looks to do the best at correctly identifying wins and losses for the home team. 

<br>

## Prediction from the test data

If we look at the 1373 games in our test data, and generate predictions for those games, we can calculate the percent of games in each year which our model correctly predicted the outcome to.  

<br>

```{r, eval = FALSE, echo = FALSE}

# Predict on 2021 season, data held out from model
pred_season <-
  holdout_test %>% 
  dplyr::bind_cols(setNames(stats::predict(log_reg_model, holdout_test), "log_reg_pred")) %>%
  dplyr::bind_cols(setNames(stats::predict(svm_rbf_model, holdout_test), "svm_pred")) %>%
  dplyr::bind_cols(setNames(stats::predict(xgb_model, holdout_test), "xgb_pred")) %>%
  dplyr::bind_cols(setNames(stats::predict(mlp_model, holdout_test), "mlp_pred")) %>%
  dplyr::bind_cols(setNames(stats::predict(log_reg_model, holdout_test, type = "prob"),
                           c("log_reg_prob_1", "log_reg_prob_0"))) %>%
  dplyr::bind_cols(setNames(stats::predict(svm_rbf_model, holdout_test, type = "prob"),
                           c("svm_prob_1", "svm_prob_0"))) %>%
  dplyr::bind_cols(setNames(stats::predict(xgb_model, holdout_test, type = "prob"),
                           c("xgb_prob_1", "xgb_prob_0"))) %>%
  dplyr::bind_cols(setNames(stats::predict(mlp_model, holdout_test, type = "prob"),
                           c("mlp_prob_1", "mlp_prob_0"))) %>%
  dplyr::select(season, week, team, opponent, fav, win,
                log_reg_pred, svm_pred, xgb_pred, mlp_pred,
                log_reg_prob_1, svm_prob_1 , xgb_prob_1, mlp_prob_1) %>% 
  dplyr::mutate(across(where(is.numeric), round, 3)) %>% 
  dplyr::mutate(
      vegas_pct = dplyr::case_when(
        win == fav ~ 1,
        win != fav ~ 0
      ),
      log_reg_pct = dplyr::case_when(
        win == log_reg_pred ~ 1,
        win != log_reg_pred ~ 0
      ),
      svm_pct = dplyr::case_when(
        win == svm_pred ~ 1,
        win != svm_pred ~ 0
      ),
      xgb_pct = dplyr::case_when(
        win == xgb_pred ~ 1,
        win != xgb_pred ~ 0
      ),
      mlp_pct = dplyr::case_when(
        win == mlp_pred ~ 1,
        win != mlp_pred ~ 0
      )
    ) %>%
  dplyr::group_by(season) %>% 
  dplyr::summarize(
    across(c(vegas_pct:mlp_pct), sum, na.rm = T),
    games = n()
    ) %>% 
  dplyr::mutate(
    vegas_pct   = vegas_pct/games,
    log_reg_pct = log_reg_pct/games,
    svm_pct     = svm_pct/games,
    xgb_pct     = xgb_pct/games,
    mlp_pct     = mlp_pct/games
  ) %>% 
  round(2) %>% 
  dplyr::select(-games)

pred_seasons_plot <-
  pred_season %>% 
  setNames(c("season", "Vegas", "Logistic Reg.", "SVM RBF", "XGBoost", "MLP")) %>% 
  tidyr::pivot_longer(c("Vegas":"MLP")) %>%
  dplyr::mutate(
    name = factor(name, levels = c("Logistic Reg.", "SVM RBF",  "XGBoost", "MLP", "Vegas"))
    ) %>%
  ggplot2::ggplot() +
  ggplot2::geom_line(ggplot2::aes(x  = season, y = value, col = name, alpha = name, size = name)) +
  ggplot2::scale_color_manual(values = c("forestgreen",  "darkorange", "red3", "dodgerblue3", "black")) +
  ggplot2::scale_size_manual(values  = c(1.2, 1.2, 1.2, 1.2, 2.5)) +
  ggplot2::scale_alpha_manual(values = c(0.7, 0.7, 0.7, 0.7, 0.7))  +
  ggplot2::scale_x_continuous(breaks = seq(2000, 2020, 2))  +
  ggplot2::scale_y_continuous(limits = c(0, 1),
    labels = function(x) paste0(x*100, "%")) +
  ggplot2::labs(
      title    = "What % of games did we correctly predict the outcome?",
      # title    = "What % of games is the outcome correctly predicted?",
      # title = "Model predictions vs. Vegas favorites",
      # title    = "Picking the game winners on the test data",
      subtitle = "Test data",
      # subtitle = "Model predictions and Vegas favorites for the test data",
      # subtitle = "Model predictions vs. Vegas favorites",
      x        = "Season",
      y        = "% of correct predictions"
    ) + 
    apatheme 
    # ggplot2::theme(
      # plot.title = element_text(size = 20)
#       axis.text.x         = ggplot2::element_text(angle = -45),
#       axis.title.x        = ggplot2::element_text(hjust=0.5,  vjust = -2),
#       axis.title.y        = ggplot2::element_text(hjust=0.5, vjust = 2)
    # )
# pred_seasons_plot
# ggsave(
#     here::here("img", "test_data_prediction_plot.png"),
#     pred_seasons_plot,
#     width  = 14,
#     height = 8
#   )
```


<center>
![](../img/test_data_prediction_plot.png)
</center>

<br>
<br>

## Holdout dataset (2021 Predictions)

Now let's apply the fitted models to the 2021 season holdout data and see how those predictions fare against Vegas' predicted winners.

We can visualize percent of games that Las Vegas got correct in 2021 and compare it to the ML models % of correct predictions. The X axis has the percent of correct predictions and the Y axis is the week of the NFL season. 

```{r, eval = FALSE, echo=FALSE}
correct_picks <-
  pred_holdout %>%
  dplyr::arrange(week) %>%
  dplyr::mutate(
      vegas_pct = dplyr::case_when(
        win == fav ~ 1,
        win != fav ~ 0
      ),
      log_reg_pct = dplyr::case_when(
        win == log_reg_pred ~ 1,
        win != log_reg_pred ~ 0
      ),
      svm_pct = dplyr::case_when(
        win == svm_pred ~ 1,
        win != svm_pred ~ 0
      ),
      xgb_pct = dplyr::case_when(
        win == xgb_pred ~ 1,
        win != xgb_pred ~ 0
      ),
      mlp_pct = dplyr::case_when(
        win == mlp_pred ~ 1,
        win != mlp_pred ~ 0
      )
    ) %>%
  dplyr::group_by(week) %>%
  dplyr::summarize(
    across(c(vegas_pct:mlp_pct), sum, na.rm = T),
    games       = n()
    ) %>%
  dplyr::mutate(
    vegas_pct    = vegas_pct/games,
    log_reg_pct  = log_reg_pct/games,
    svm_pct      = svm_pct/games,
    xgb_pct      = xgb_pct/games,
    mlp_pct      = mlp_pct/games
  ) %>%
  round(2) %>%
  dplyr::select(-games)
```

```{r, eval = FALSE, echo=FALSE}
# % of correct picks by week
week_picks <- 
  correct_picks %>% 
  dplyr::filter(week <= 21) %>%
  setNames(c("week", "Vegas", "Logistic Reg.", "SVM", "XGBoost", "MLP")) %>%
  tidyr::pivot_longer(c("Logistic Reg.":"MLP")) %>%
  dplyr::mutate(
    name = factor(name, levels = c("Logistic Reg.", "SVM",  "XGBoost", "MLP"))
    ) 

holdout_grid_plot <- 
  ggplot2::ggplot() +
  ggplot2::geom_line(data = week_picks, aes(x = week, y = Vegas), col = "black", alpha = 0.6, size = 2.5) +
  ggplot2::geom_line(data = week_picks, aes(x = week, y = value, col = name), size = 1.5) +
  ggplot2::facet_grid(name~.) +
  ggplot2::scale_color_manual(values = c("forestgreen",  "darkorange", "red3", "dodgerblue3")) +
  ggplot2::scale_x_continuous(breaks = seq(2, 22, 1)) +
  ggplot2::scale_y_continuous(limits = c(0, 1),
    labels = function(x) paste0(x*100, "%")
    ) +
  ggplot2::labs(
      title    = "% of correct picks by week (2021)",
      subtitle = "Model predictions vs. Vegas (black line)",
      x = "Week",
      y = "% of correct predictions"
    ) + 
    apatheme +
    ggplot2::theme(
      # axis.title.x        = ggplot2::element_text(hjust=0.5,  vjust = -2),
      # axis.title.y        = ggplot2::element_text(hjust=0.5, vjust = 2),
      panel.grid.minor.x = element_blank()
    )
holdout_grid_plot
ggsave(
  here::here("img", "holdout_facet_line_plot.png"),
  holdout_grid_plot,
  width  = 12,
  height = 10
  )

# # % of correct picks by week
# point_picks_plot <- 
  lollipop_df <- 
    correct_picks %>%
  dplyr::filter(week <= 21) %>%
  setNames(c("week", "Vegas", "Logistic Reg.", "SVM", "XGBoost", "MLP")) %>%
  tidyr::pivot_longer(c("Logistic Reg.":"MLP")) %>%
  tidyr::pivot_longer(c("Vegas"), names_to = "vegas_name", values_to = "vegas_val") %>%
  # tidyr::pivot_longer(c("Vegas":"MLP")) %>%
  dplyr::mutate(
    name = factor(name, levels = c("Logistic Reg.", "SVM",  "XGBoost", "MLP")),
    vegas_name = factor(vegas_name, levels = c("Vegas")),
    )
  # tidyr::pivot_longer(c(vegas_pct:xgb_pct)) %>%
 lollipop_points_plot <-
  # lollipop_wrap_plot <-
   lollipop_df %>% 
    ggplot2::ggplot() +
    ggplot2::geom_segment(aes(x = week, xend = week, y = value, yend = vegas_val), col = "black", alpha = 0.6, size = 1) +
    ggplot2::geom_point(aes(x = week, y = value, col = name), size = 5) +
    ggplot2::facet_wrap(~name) +
    ggplot2::geom_point(aes(x = week, y = vegas_val, col = vegas_name), alpha = 0.7, size = 5) +
    ggplot2::scale_color_manual(values = c("forestgreen",  "dodgerblue3", "darkorange", "black", "red3")) + 
    # ggplot2::scale_size_manual(values = c(4, 4, 4, 4, 6)) +
    # ggplot2::scale_alpha_manual(values = c(0.8, 0.8, 0.8, 0.8, 0.8)) +
    ggplot2::scale_x_continuous(breaks = seq(2, 22, 1)) +
    ggplot2::scale_y_continuous(limits = c(0, 1),
      labels = function(x) paste0(x*100, "%")
      ) +
    # ggplot2::geom_line(aes(x = week, y = value, col = name)) 
    ggplot2::labs(
        title    = "% of correct picks by week (2021)",
        subtitle = "Model predictions vs. Vegas",
        x = "Week",
        y = "% of correct predictions"
      ) +
      apatheme +
      ggplot2::theme(
        axis.title.x        = ggplot2::element_text(hjust=0.5,  vjust = -2),
        axis.title.y        = ggplot2::element_text(hjust=0.5, vjust = 2),
        panel.grid.minor.x = ggplot2::element_blank()
      )

# ggsave(
#     here::here("img", "holdout_lollipop_wrap_plot.png"),
#     lollipop_wrap_plot,
#     width  = 12,
#     height = 10
#   )
# ggsave(
#     here::here("img", "holdout_lollipop_plot.png"),
#     lollipop_points_plot,
#     width  = 12,
#     height = 10
#   )
```

<br>

<center>
![](../img/holdout_facet_line_plot.png)
</center>

Looks like our predictions are right in line with Vegas, in some weeks we do better than Vegas are predicting the correct game outcomes and some weeks we do worse. 

```{r, eval = FALSE, echo=FALSE}
# Predict on 2021 season, data held out from model
# pred_holdout <-
#   holdout_df %>% 
#   dplyr::bind_cols(setNames(stats::predict(log_reg_model, holdout_df), "log_reg_pred")) %>%
# 
#   dplyr::bind_cols(setNames(stats::predict(svm_rbf_model, holdout_df), "svm_pred")) %>%
#   dplyr::bind_cols(setNames(stats::predict(xgb_model, holdout_df), "xgb_pred")) %>%
#     dplyr::bind_cols(setNames(stats::predict(mlp_model, holdout_df), "mlp_pred")) %>%
#   dplyr::bind_cols(setNames(stats::predict(log_reg_model, holdout_df, type = "prob"),
#                            c("log_reg_prob_1", "log_reg_prob_0"))) %>%
#   dplyr::bind_cols(setNames(stats::predict(svm_rbf_model, holdout_df, type = "prob"),
#                            c("svm_prob_1", "svm_prob_0"))) %>%
#   dplyr::bind_cols(setNames(stats::predict(xgb_model, holdout_df, type = "prob"),
#                            c("xgb_prob_1", "xgb_prob_0"))) %>%
#   dplyr::bind_cols(setNames(stats::predict(mlp_model, holdout_df, type = "prob"),
#                            c("mlp_prob_1", "mlp_prob_0"))) %>%
#   dplyr::select(season, week, team, opponent, fav, win,
#                 log_reg_pred, svm_pred, xgb_pred, mlp_pred,
#                 log_reg_prob_1, svm_prob_1 , xgb_prob_1, mlp_prob_1) %>% 
#   dplyr::mutate(across(where(is.numeric), round, 3))

```

```{r, eval = FALSE, echo=FALSE}
# # Clean data for table, set colors 
# pred_table <- 
#   pred_holdout %>% 
#   dplyr::mutate(
#     actual_outcome = dplyr::case_when(
#       win == 1 ~ "win",
#       win == 0 ~ "loss"
#     ),
#     vegas_pred = dplyr::case_when(
#       fav == 1 ~ "win",
#       fav == 0 ~ "loss"
#     ),
#     log_reg_pred = dplyr::case_when(
#       log_reg_pred == 1 ~ "win",
#       log_reg_pred == 0 ~ "loss"
#     ),
#     svm_pred = dplyr::case_when(
#       svm_pred == 1 ~ "win",
#       svm_pred == 0 ~ "loss"
#     ),
#     xgb_pred = dplyr::case_when(
#       xgb_pred == 1 ~ "win",
#       xgb_pred == 0 ~ "loss"
#       ),
#     mlp_pred = dplyr::case_when(
#       mlp_pred == 1 ~ "win",
#       mlp_pred == 0 ~ "loss"
#       )
#     ) %>% 
#   dplyr::arrange(week) %>% 
#   dplyr::select(week, home_team = team, away_team = opponent, actual_outcome, vegas_pred,
#                 log_reg_pred,
#                 svm_pred,
#                 xgb_pred,
#                 mlp_pred
#                 )
```

<br>

The table below shows the actual outcomes of the 2021 season,  the favored team according to Las Vegas, and the prediction's from my fitted models. Green highlighted cells indicate that the prediction lined up with the actual game outcome, and red highlighted cells mean that the prediction was incorrect. The table is from the home team's persepctive (i.e. a win in the actual_outcome column means the home team won).

First we'll can check out one of the weeks that our models outperformed Vegas.

```{r, eval = TRUE, echo=FALSE}
# # Color code TP and TN as green and FP and FN as red
# colormatrix  <- ifelse(pred_table[, c(5, 6, 7, 8, 9)] == pred_table[, c(4, 4, 4, 4, 4)], "#85E088", "indianred2")
# 
# # Flextable with color coding
# pred_table %>% 
#   flextable::flextable() %>% 
#   flextable::bg(j = c(5:9), bg = colormatrix) %>% 
#   flextable::add_header_row(
#     values    = c(" ", "Vegas", "Models"), 
#     colwidths = c(4, 1, 4)
#     ) %>% 
#   flextable::theme_box() %>% 
#   flextable::align(align = "center", part = "all")
```

## Good  week of predictions
All of our models did better than Vegas in week 12, 

```{r, eval = TRUE, echo = TRUE, fig.align="center", fig.width=10, fig.height=10}
# A week with more preditions correct than Las Vegas
good_week <- 
  pred_table %>% 
  dplyr::filter(week == 12)

# Color code TP and TN as green and FP and FN as red
good_week_color <- ifelse(good_week[, c(5, 6, 7, 8, 9)] == good_week[, c(4, 4, 4, 4, 4)], "#85E088", "indianred2")

# Flextable with color coding
good_week %>% 
  flextable::flextable() %>% 
  flextable::bg(j = c(5:9), bg = good_week_color) %>% 
  flextable::add_header_row(
    values    = c(" ", "Vegas", "Models"), # values = c(" ","Vegas Predictions", "Model Predictions"),
    colwidths = c(4, 1, 4)
    # values    = c(" ", "Vegas / Model Predictions"), colwidths = c(4, 4)
    ) %>% 
  flextable::theme_box() %>% 
  flextable::align(align = "center", part = "all")
```

<br>
<br>

And now a bad week...

## Bad week of predictions
```{r, eval = TRUE, echo = TRUE, fig.align="center", fig.width=12, fig.height=10}
# A week with more preditions correct than Las Vegas
bad_week <- 
  pred_table %>% 
  dplyr::filter(week == 15)

# Color code TP and TN as green and FP and FN as red
bad_week_color <- ifelse(bad_week[, c(5, 6, 7, 8, 9)] == bad_week[, c(4, 4, 4, 4, 4)], "#85E088", "indianred2")

# Flextable with color coding
bad_week %>% 
  flextable::flextable() %>% 
  flextable::bg(j = c(5:9), bg = bad_week_color) %>% 
  flextable::add_header_row(
    values    = c(" ", "Vegas", "Models"), 
    colwidths = c(4, 1, 4)
    ) %>% 
  flextable::theme_box() %>% 
  flextable::align(align = "center", part = "all")
```

<br>
<br>

---

# **Conclusion**

We did it, we implemented NFL data into a Machine learning workflow and generated some reasonably accurate predictions!

To recap what I did:
- Started with raw NFL play-by-play data
- Cleaned and processed the data into a model-ready format
- Selected a handful of ML models,
- trained and tested the models with our prepped data,
- Split our data into training and testing datasets
- Trained our models using the training data and evaulated them using 10 fold cross validation.
- Determined optimal hyperparameters and used these to refit our models on the entire dataset
- Generated predictions and compared our predictions to the predicted favorites according to Las Vegas

I'm not a huge sports bettor, but I really just wanted to make a model that could potentially make me money if I was. 
















